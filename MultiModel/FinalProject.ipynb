{"cells":[{"cell_type":"code","execution_count":1,"id":"6daa600e-eb53-4997-ab63-4506f8ff18d7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6daa600e-eb53-4997-ab63-4506f8ff18d7","executionInfo":{"status":"ok","timestamp":1765358606149,"user_tz":600,"elapsed":26787,"user":{"displayName":"Yuzhi Han","userId":"01459783184240571767"}},"outputId":"7e4578ba-b65a-4034-9967-e8f50486d17a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyedflib\n","  Downloading pyedflib-0.1.42-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.12/dist-packages (from pyedflib) (2.0.2)\n","Downloading pyedflib-0.1.42-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m2.5/2.8 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyedflib\n","Successfully installed pyedflib-0.1.42\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.9.0+cu126)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.5.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->torchvision) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->torchvision) (3.0.3)\n"]}],"source":["!pip install pyedflib\n","!pip install torchvision\n","import pyedflib\n","import numpy as np\n","import cv2\n","import dlib\n","import os\n","import pickle\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm\n"]},{"cell_type":"code","execution_count":null,"id":"5102bdb9-03c0-4650-8a41-a6f5566dd6c6","metadata":{"id":"5102bdb9-03c0-4650-8a41-a6f5566dd6c6"},"outputs":[],"source":["def load_psg(edf_path, channels):\n","    # Open the EDF file\n","    edf_reader = pyedflib.EdfReader(edf_path)\n","    # Get the number of signals in the file\n","    num_signals = edf_reader.signals_in_file\n","    # Retrieve signal labels and data\n","    signal_labels = edf_reader.getSignalLabels()\n","    signal_data = {}\n","    for signal, label in enumerate(signal_labels):\n","        data = edf_reader.readSignal(signal)\n","        signal_data[label] = data\n","    edf_reader.close()\n","    psg = []\n","    for c in channels:\n","        psg.append(signal_data[c])\n","    psg = np.stack(psg, axis=1)\n","    return psg"]},{"cell_type":"code","execution_count":null,"id":"76477077-a484-4ae1-80fb-70319bfba377","metadata":{"id":"76477077-a484-4ae1-80fb-70319bfba377"},"outputs":[],"source":["def extract_face_frames(video_path, downsample_fps):\n","    detector = dlib.get_frontal_face_detector()\n","    vid = cv2.VideoCapture(video_path)\n","    fps = vid.get(cv2.CAP_PROP_FPS)\n","    step = fps / downsample_fps\n","    success = True\n","    face_frames = []\n","    frame_times = []\n","    frame_idx = 0\n","    while success:\n","        success, image = vid.read() # Read frame\n","        if success:\n","            if frame_idx % step != 0:\n","                frame_idx += 1\n","                continue\n","            t = frame_idx / fps\n","            frame_idx += 1\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","            faces = detector(image, 1)\n","            if len(faces) == 0:\n","            # no face detected in this frame -> just skip it\n","                continue\n","            face = faces[0]\n","            x1 = face.left()\n","            y1 = face.top()\n","            x2 = face.right()\n","            y2 = face.bottom()\n","\n","            # clip to bounds\n","            h, w = image.shape\n","            x1 = max(0, x1)\n","            y1 = max(0, y1)\n","            x2 = min(w, x2)\n","            y2 = min(h, y2)\n","\n","            face_crop = image[y1:y2, x1:x2]\n","\n","            # resize to 75x75\n","            face_resized = cv2.resize(face_crop, (75, 75))\n","\n","            # normalize to [0,1]\n","            face_norm = face_resized.astype(np.float32) / 255.0\n","\n","            face_frames.append(face_norm)  # (75,75)\n","            frame_times.append(t)\n","\n","    vid.release()\n","    return face_frames, frame_times"]},{"cell_type":"code","execution_count":null,"id":"b0590f30-c78a-4558-87b6-abfb4c9bf220","metadata":{"id":"b0590f30-c78a-4558-87b6-abfb4c9bf220"},"outputs":[],"source":["def build_samples(psg, face_frames, frame_times, label):\n","    samples = []\n","    window_length = 1\n","    psg_fr = 512\n","    frame_times = np.array(frame_times)\n","    for index, time in enumerate(frame_times):\n","        if time >= 1:\n","            end_index = int(np.round(time * psg_fr))\n","            start_index = int(np.round((time - window_length) * psg_fr))\n","            psg_window = psg[start_index:end_index,:]\n","            start_time = time - window_length\n","            idxs = np.where((frame_times >= start_time) & (frame_times <= time))[0]\n","            avg_face = np.stack([face_frames[idx] for idx in idxs], axis=0).mean(axis=0)\n","            samples.append([avg_face, psg_window, label])\n","    return samples\n"]},{"cell_type":"code","execution_count":null,"id":"c4612ab7-a2a8-4ad5-b1c0-907b99479c72","metadata":{"id":"c4612ab7-a2a8-4ad5-b1c0-907b99479c72"},"outputs":[],"source":["def KSS_class(KSS):\n","    return 1 if KSS >= 6 else 0\n"]},{"cell_type":"code","execution_count":null,"id":"d964da25-52ff-46a2-b50c-eea6a7efb39d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d964da25-52ff-46a2-b50c-eea6a7efb39d","executionInfo":{"status":"ok","timestamp":1765254071704,"user_tz":360,"elapsed":47,"user":{"displayName":"Byron Hart","userId":"11853666408218305808"}},"outputId":"c5d4a2ca-822f-4d40-c6cc-17597cdd73a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Skipping 1-1: missing EDF or video\n","Skipping 1-2: missing EDF or video\n","Skipping 1-3: missing EDF or video\n","Skipping 2-1: missing EDF or video\n","Skipping 2-2: missing EDF or video\n","Skipping 2-3: missing EDF or video\n","Skipping 3-1: missing EDF or video\n","Skipping 3-2: missing EDF or video\n","Skipping 3-3: missing EDF or video\n","Skipping 4-1: missing EDF or video\n","Skipping 4-2: missing EDF or video\n","Skipping 4-3: missing EDF or video\n","Skipping 5-1: missing EDF or video\n","Skipping 5-2: missing EDF or video\n","Skipping 5-3: missing EDF or video\n","Skipping 6-1: missing EDF or video\n","Skipping 6-2: missing EDF or video\n","Skipping 6-3: missing EDF or video\n","Skipping 7-1: missing EDF or video\n","Skipping 7-2: missing EDF or video\n","Skipping 7-3: missing EDF or video\n","Skipping 8-1: missing EDF or video\n","Skipping 8-2: missing EDF or video\n","Skipping 8-3: missing EDF or video\n","Skipping 9-1: missing EDF or video\n","Skipping 9-2: missing EDF or video\n","Skipping 9-3: missing EDF or video\n","Skipping 10-1: missing EDF or video\n","Skipping 10-2: missing EDF or video\n","Skipping 10-3: missing EDF or video\n","Skipping 11-1: missing EDF or video\n","Skipping 11-2: missing EDF or video\n","Skipping 11-3: missing EDF or video\n","Skipping 12-1: missing EDF or video\n","Skipping 12-2: missing EDF or video\n","Skipping 12-3: missing EDF or video\n","Skipping 13-1: missing EDF or video\n","Skipping 13-2: missing EDF or video\n","Skipping 13-3: missing EDF or video\n","Skipping 14-1: missing EDF or video\n","Skipping 14-2: missing EDF or video\n","Skipping 14-3: missing EDF or video\n","Subject 1: total 0 samples\n","Subject 2: total 0 samples\n","Subject 3: total 0 samples\n","Subject 4: total 0 samples\n","Subject 5: total 0 samples\n","Subject 6: total 0 samples\n","Subject 7: total 0 samples\n","Subject 8: total 0 samples\n","Subject 9: total 0 samples\n","Subject 10: total 0 samples\n","Subject 11: total 0 samples\n","Subject 12: total 0 samples\n","Subject 13: total 0 samples\n","Subject 14: total 0 samples\n"]}],"source":["psg_dir = r\"C:\\Users\\gojas\\Downloads\\DROZY\\DROZY\\psg\"\n","video_dir = r\"C:\\Users\\gojas\\Downloads\\DROZY\\DROZY\\videos_i8\"\n","channels = ['Fz','Cz','C3','C4','Pz','EOG-V','EOG-H','EMG','ECG']\n","KSS_values = [3, 6, 7,\n","              3, 7, 6,\n","              2, 3, 4,\n","              4, 8, 9,\n","              3, 7, 8,\n","              2, 3, 7,\n","              0, 4, 9,\n","              2, 6, 8,\n","              2, 6, 8,\n","              3, 6, 7,\n","              4, 7, 7,\n","              2, 5, 6,\n","              6, 3, 7,\n","              5, 7, 8]\n","\n","subjects = np.arange(1,15)\n","subject_samples = {subj: [] for subj in subjects}\n","tests = [1, 2, 3]\n","\n","sessions = []\n","for subject in subjects:\n","    for test in tests:\n","        sessions.append((subject, test, f\"{subject}-{test}\"))\n","\n","session_labels = {}\n","for session, KSS in zip(sessions, KSS_values):\n","    session_labels[session[2]] = KSS_class(KSS)\n","for subject,test,session in sessions:\n","    edf_path = os.path.join(psg_dir,  session + \".edf\")\n","    vid_path = os.path.join(video_dir, session + \".mp4\")\n","\n","    if not os.path.exists(edf_path) or not os.path.exists(vid_path):\n","        print(f\"Skipping {session}: missing EDF or video\")\n","        continue\n","    if session not in session_labels:\n","        print(f\"Skipping {session}: no KSS label\")\n","        continue\n","\n","    label = session_labels[session]\n","    print(f\"Processing session {session} (subject {subject}, test {test}) ...\")\n","\n","    psg = load_psg(edf_path, channels)\n","    face_frames, frame_times = extract_face_frames(vid_path, 1)\n","    if len(face_frames) == 0:\n","        print(f\"  No faces in {session}, skipping\")\n","        continue\n","    session_samples = build_samples(psg, face_frames, frame_times, label)\n","\n","    print(f\"{session}: {len(session_samples)} samples\")\n","    subject_samples[subject].extend(session_samples)\n","\n","for subj in subjects:\n","    print(f\"Subject {subj}: total {len(subject_samples[subj])} samples\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"610d1f39-b341-4871-9bf9-bb54d83862d0","metadata":{"id":"610d1f39-b341-4871-9bf9-bb54d83862d0"},"outputs":[],"source":["expected_len = 512\n","num_removed = 0\n","\n","for subj in list(subject_samples.keys()):\n","    clean_list = []\n","    for face, phys, label in subject_samples[subj]:\n","        if phys.shape[0] == expected_len:\n","            clean_list.append([face, phys, label])\n","        else:\n","            num_removed += 1\n","    subject_samples[subj] = clean_list"]},{"cell_type":"code","execution_count":null,"id":"689fe835-44af-4e9e-8180-e308c98b4e73","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"689fe835-44af-4e9e-8180-e308c98b4e73","executionInfo":{"status":"ok","timestamp":1765254063121,"user_tz":360,"elapsed":14,"user":{"displayName":"Byron Hart","userId":"11853666408218305808"}},"outputId":"7a848e43-f9e3-4511-f1b8-bb0580a83945"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saved subject_samples to subject_samples.pkl\n"]}],"source":["save_path = \"subject_samples.pkl\"  # you can put a full path if you want\n","\n","with open(save_path, \"wb\") as f:\n","    pickle.dump(subject_samples, f)\n","\n","print(\"Saved subject_samples to\", save_path)"]},{"cell_type":"code","execution_count":null,"id":"1b9f9477-2df8-4803-b1c6-b2801557bbba","metadata":{"id":"1b9f9477-2df8-4803-b1c6-b2801557bbba"},"outputs":[],"source":["subject_ids = list(subject_samples.keys())   # [1,2,...,14]\n","\n","np.random.seed(42)\n","\n","np.random.shuffle(subject_ids)\n","\n","trainval_subjects = subject_ids[:13]   # first 13\n","test_subjects      = subject_ids[13]    # last one\n","\n","train_subjects = trainval_subjects[:9]   # first 9\n","val_subjects   = trainval_subjects[9:]   # remaining 4\n","\n"]},{"cell_type":"code","execution_count":null,"id":"d1dee99f-6e56-460c-9d8e-a2eb764c0262","metadata":{"id":"d1dee99f-6e56-460c-9d8e-a2eb764c0262"},"outputs":[],"source":["def gather_samples_for_subjects(subject_ids, subject_samples):\n","    all_samples = []\n","    for s in subject_ids:\n","        all_samples.extend(subject_samples[s])\n","    return all_samples\n","\n","train_samples = gather_samples_for_subjects(train_subjects, subject_samples)\n","val_samples   = gather_samples_for_subjects(val_subjects, subject_samples)\n","test_samples  = subject_samples[test_subjects]\n"]},{"cell_type":"code","execution_count":null,"id":"ea4d28a1-39a0-4b16-b523-12159acfc2a1","metadata":{"id":"ea4d28a1-39a0-4b16-b523-12159acfc2a1"},"outputs":[],"source":["class DrowsinessDataset(Dataset):\n","    def __init__(self, samples):\n","        self.samples = samples\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        face, phys, label = self.samples[idx]\n","\n","        face_tensor = torch.from_numpy(face).float().unsqueeze(0)\n","\n","        phys_tensor = torch.from_numpy(phys).float()\n","\n","        label_tensor = torch.tensor(label, dtype=torch.long)\n","\n","        return face_tensor, phys_tensor, label_tensor\n"]},{"cell_type":"code","execution_count":null,"id":"c08419d0-7435-40d5-a317-75c04198659a","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"c08419d0-7435-40d5-a317-75c04198659a","executionInfo":{"status":"error","timestamp":1765253690261,"user_tz":360,"elapsed":72,"user":{"displayName":"Byron Hart","userId":"11853666408218305808"}},"outputId":"62e1a573-9d42-4bd2-b564-41046b254463"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"num_samples should be a positive integer value, but got num_samples=0","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1075871104.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_dataset\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mDrowsinessDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mval_loader\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest_loader\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             )\n","\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"]}],"source":["batch_size = 32\n","\n","train_dataset = DrowsinessDataset(train_samples)\n","val_dataset   = DrowsinessDataset(val_samples)\n","test_dataset  = DrowsinessDataset(test_samples)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n","test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"id":"da534e5d-3150-4db8-bab0-d2af4873ef36","metadata":{"id":"da534e5d-3150-4db8-bab0-d2af4873ef36"},"outputs":[],"source":["\n","class FaceResNetEncoder(nn.Module):\n","    def __init__(self, feature_dim=512):\n","        super().__init__()\n","        # base ResNet18\n","        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","        in_feats = self.backbone.fc.in_features\n","        # remove final classifier\n","        self.backbone.fc = nn.Identity()\n","        # projection head\n","        self.project = nn.Linear(in_feats, feature_dim)\n","\n","    def forward(self, x):\n","        x = x.repeat(1, 3, 1, 1)\n","        feats = self.backbone(x)         # (B, in_feats)\n","        feats = self.project(feats)      # (B, feature_dim)\n","        return feats\n"]},{"cell_type":"code","execution_count":null,"id":"a09cfaf9-91d5-4412-a932-95154b1bc0a9","metadata":{"id":"a09cfaf9-91d5-4412-a932-95154b1bc0a9"},"outputs":[],"source":["class PhysLSTMEncoder(nn.Module):\n","    def __init__(self, input_size=9, hidden_size=128, num_layers=1, out_dim=512):\n","        super().__init__()\n","        self.lstm = nn.LSTM(\n","            input_size=input_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True\n","        )\n","        self.project = nn.Linear(hidden_size, out_dim)\n","\n","    def forward(self, x):\n","        out, (h_n, c_n) = self.lstm(x)\n","        last_hidden = h_n[-1]\n","        feats = self.project(last_hidden)\n","        return feats\n"]},{"cell_type":"code","execution_count":null,"id":"dab41cb6-61ac-42c7-bff3-e76760b4442a","metadata":{"id":"dab41cb6-61ac-42c7-bff3-e76760b4442a"},"outputs":[],"source":["seed = 1000\n","if torch.cuda.is_available() and try_cuda:\n","    cuda = True\n","    torch.cuda.manual_seed(seed)\n","else:\n","    cuda = False\n","    torch.manual_seed(seed)\n","device = torch.device(\"cuda\" if cuda else \"cpu\")\n","face_encoder = FaceResNetEncoder(feature_dim=512)\n","phys_encoder = PhysLSTMEncoder(input_size=9, hidden_size=128, out_dim=512)\n","face_encoder.to(device)\n","phys_encoder.to(device)"]},{"cell_type":"code","execution_count":null,"id":"65543c1e-7d92-4c76-8204-446c7bb727a5","metadata":{"id":"65543c1e-7d92-4c76-8204-446c7bb727a5"},"outputs":[],"source":["class FeatureCoupledNet(nn.Module):\n","    def __init__(self, num_channels=9, num_classes=2,\n","                 face_feat_dim=512, phys_out_dim=512, dropout=0.4):\n","        super().__init__()\n","        self.face_encoder = FaceResNetEncoder(feature_dim=face_feat_dim)\n","        self.phys_encoder = PhysLSTMEncoder(input_size=num_channels,\n","                                            hidden_size=256,\n","                                            out_dim=phys_out_dim)\n","\n","        self.fc1 = nn.Linear(face_feat_dim, 512)\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc_out = nn.Linear(512, num_classes)\n","\n","    def minmax_norm(self, x, eps=1e-6):\n","        x_min = x.min(dim=1, keepdim=True)[0]\n","        x_max = x.max(dim=1, keepdim=True)[0]\n","        denom = (x_max - x_min).clamp(min=eps)\n","        return (x - x_min) / denom\n","\n","    def forward(self, face, phys):\n","        f_img  = self.face_encoder(face)\n","        f_phys = self.phys_encoder(phys)\n","\n","        f_img_norm  = self.minmax_norm(f_img)\n","        f_phys_norm = self.minmax_norm(f_phys)\n","\n","        coupled = f_img_norm * f_phys_norm\n","\n","        h = F.relu(self.fc1(coupled))\n","        h=self.dropout(h)\n","        logits = self.fc_out(h)\n","        return logits\n"]},{"cell_type":"code","execution_count":null,"id":"7bc43076-9a72-4994-8b48-d7cd0f7ddbaa","metadata":{"id":"7bc43076-9a72-4994-8b48-d7cd0f7ddbaa"},"outputs":[],"source":["model = FeatureCoupledNet(num_channels=9, num_classes=2).to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3,weight_decay=1e-4)\n"]},{"cell_type":"code","execution_count":null,"id":"8dc66beb-582c-4949-9c07-2f0700c8edc8","metadata":{"id":"8dc66beb-582c-4949-9c07-2f0700c8edc8"},"outputs":[],"source":["def train_one_epoch(model, loader, optimizer, criterion, device, epoch=None):\n","    model.train()\n","    total_loss = 0.0\n","    total_correct = 0\n","    total_count = 0\n","    loop = tqdm(loader, desc=f\"Train Epoch {epoch}\", leave=False)\n","    for faces, phys, labels in loop:\n","        faces = faces.to(device)\n","        phys  = phys.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","        logits = model(faces, phys)           # (B, num_classes)\n","        loss = criterion(logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * labels.size(0)\n","        preds = logits.argmax(dim=1)\n","        total_correct += (preds == labels).sum().item()\n","        total_count += labels.size(0)\n","        loop.set_postfix(loss=loss.item())\n","\n","    return total_loss / total_count, total_correct / total_count\n","\n","\n","def eval_one_epoch(model, loader, criterion, device, epoch=None, phase=\"Val\"):\n","    model.eval()\n","    total_loss = 0.0\n","    total_correct = 0\n","    total_count = 0\n","    loop = tqdm(loader, desc=f\"{phase} Epoch {epoch}\", leave=False)\n","    with torch.no_grad():\n","        for faces, phys, labels in loop:\n","            faces = faces.to(device)\n","            phys  = phys.to(device)\n","            labels = labels.to(device)\n","\n","            logits = model(faces, phys)\n","            loss = criterion(logits, labels)\n","\n","            total_loss += loss.item() * labels.size(0)\n","            preds = logits.argmax(dim=1)\n","            total_correct += (preds == labels).sum().item()\n","            total_count += labels.size(0)\n","            loop.set_postfix(loss=loss.item())\n","\n","    return total_loss / total_count, total_correct / total_count\n"]},{"cell_type":"code","execution_count":null,"id":"61908cec-d1dd-4b93-83ec-f06d08ff468e","metadata":{"id":"61908cec-d1dd-4b93-83ec-f06d08ff468e"},"outputs":[],"source":["num_epochs = 10  # or more\n","\n","for epoch in range(1, num_epochs + 1):\n","    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)\n","    val_loss, val_acc     = eval_one_epoch(model, val_loader,   criterion, device, epoch, phase=\"Val\")\n","\n","    print(f\"Epoch {epoch:02d} | \"\n","          f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n","          f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f}\")\n"]},{"cell_type":"code","execution_count":null,"id":"98b5f257-fca0-44f1-8a75-cd39867a8c2a","metadata":{"id":"98b5f257-fca0-44f1-8a75-cd39867a8c2a"},"outputs":[],"source":["test_loss, test_acc = eval_one_epoch(model, test_loader, criterion, device)\n","print(f\"Test | loss={test_loss:.4f}, acc={test_acc:.3f}\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python [conda env:base] *","language":"python","name":"conda-base-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.5"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}